{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":114393,"status":"ok","timestamp":1666537009613,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"p7EijYzYYI1s","outputId":"59e026d4-3c37-44ff-bbea-8f7838eb8218"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.9.0\n","  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n","\u001b[K     |████████████████████████████████| 831.4 MB 2.6 kB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0) (4.1.1)\n","Installing collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.1+cu113\n","    Uninstalling torch-1.12.1+cu113:\n","      Successfully uninstalled torch-1.12.1+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\n","torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\n","torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\u001b[0m\n","Successfully installed torch-1.9.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchtext==0.10.0\n","  Downloading torchtext-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 19.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n","Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.9.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.1.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2022.9.24)\n","Installing collected packages: torchtext\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.13.1\n","    Uninstalling torchtext-0.13.1:\n","      Successfully uninstalled torchtext-0.13.1\n","Successfully installed torchtext-0.10.0\n"]}],"source":["# !pip install torch==1.9.0\n","# !pip install torchtext==0.10.0"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23244,"status":"ok","timestamp":1666537394067,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"bjwjY2rHbLyj","outputId":"4e5d3479-33d4-4896-9dca-4c67d88e83c5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29096,"status":"ok","timestamp":1666537038705,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"s61LmfKGRZxH","outputId":"64dcb929-6e5f-44e9-f9eb-c85b62a3b413"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.4)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n","\u001b[K     |████████████████████████████████| 5.3 MB 19.9 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 43.6 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 63.7 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sklearn\n","  Downloading sklearn-0.0.tar.gz (1.1 kB)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.7.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n","Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.6)\n","Building wheels for collected packages: sklearn\n","  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=16c2cbbf2f9fb788caf3db4abb015e4e650809c19f45a487ea8e3ae5581d0b27\n","  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n","Successfully built sklearn\n","Installing collected packages: sklearn\n","Successfully installed sklearn-0.0\n"]}],"source":["# !pip install numpy\n","# !pip install pandas\n","# !pip install nltk\n","# !pip install transformers\n","# !pip install sklearn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"btbe1xilpGYk"},"outputs":[],"source":["# !nvidia-smi"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":4003,"status":"ok","timestamp":1666537333028,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"SRf6WoRqRWrs"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import nltk\n","import torch\n","from transformers import BertTokenizer\n","from torchtext.legacy import data\n","from sklearn.model_selection import train_test_split\n","import transformers\n","from transformers import BertModel\n","import torch.nn as nn\n","import torch.optim as optim\n","import math\n","import time\n","import pickle\n","\n","#from transformers import *\n"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":5005,"status":"ok","timestamp":1666539524326,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"XItXN57HRWrv"},"outputs":[],"source":["train_df = pd.read_feather('/content/drive/MyDrive/duplicate/train_dataset.feather')\n","test_df = pd.read_feather('/content/drive/MyDrive/duplicate/test_dataset.feather')\n","val_df = pd.read_feather('/content/drive/MyDrive/duplicate/validation_dataset.feather')"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":1300,"status":"ok","timestamp":1666539570876,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"j-zQnO0xRWrv"},"outputs":[],"source":["def trim_sentence(sent):\n","    try:\n","        sent = sent.split()\n","        sent = sent[:128]\n","        return \" \".join(sent)\n","    except:\n","        return sent\n","\n","train_df['q1_trimmed'] = train_df['q1_cleaned'].apply(lambda x: trim_sentence(x))\n","train_df['q2_trimmed'] = train_df['q2_cleaned'].apply(lambda x: trim_sentence(x))\n","\n","val_df['q1_trimmed'] = val_df['q1_cleaned'].apply(lambda x: trim_sentence(x))\n","val_df['q2_trimmed'] = val_df['q2_cleaned'].apply(lambda x: trim_sentence(x))\n","\n","test_df['q1_trimmed'] = test_df['q1_cleaned'].apply(lambda x: trim_sentence(x))\n","test_df['q2_trimmed'] = test_df['q2_cleaned'].apply(lambda x: trim_sentence(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GSNVWbUgRWrw"},"outputs":[],"source":["# def common_words(row):\n","#     w1 = set(map(lambda word: word.lower().strip(), row['q1_cleaned'].split(\" \")))\n","#     w2 = set(map(lambda word: word.lower().strip(), row['q2_cleaned'].split(\" \")))    \n","#     return len(w1 & w2)\n","    \n","# df['jaccard_dist'] = nltk.jaccard_distance(set(df['q1_cleaned']), set(df['q2_cleaned']))\n","# df['common_words'] = df.apply(common_words, axis=1)\n","# df['common_ratio'] = df.apply(lambda row: row['common_words'] / (len(row['q1_cleaned']) + len(row['q2_cleaned'])), axis=1)"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":497,"status":"ok","timestamp":1666539579206,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"OV0A_8OPRWrw"},"outputs":[],"source":["# Masking the tokens to feed into BERT\n","def sent1_token_type(sentence):\n","    try:\n","        return [0]* len(sentence)\n","    except:\n","        return []\n","    \n","#Get list of 1s\n","def sent2_token_type(sentence):\n","    try:\n","        return [1]* len(sentence)\n","    except:\n","        return []\n","\n","#combine from lists\n","def combine_seq(seq):\n","    return \" \".join(seq)\n","\n","#combines from lists of int\n","def combine_mask(mask):\n","    mask = list(map(str, mask))\n","    return \" \".join(mask)\n","\n","#convert attention mask back to list of int\n","def convert_mask(tok_ids):\n","    tok_ids = [int(x) for x in tok_ids]\n","    return tok_ids"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":630,"status":"ok","timestamp":1666539582597,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"TqDH2cJrRWrw"},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","cls_token = tokenizer.cls_token\n","sep_token = tokenizer.sep_token\n","pad_token = tokenizer.pad_token\n","unk_token = tokenizer.unk_token\n","cls_token_idx = tokenizer.cls_token_id\n","sep_token_idx = tokenizer.sep_token_id\n","pad_token_idx = tokenizer.pad_token_id\n","unk_token_idx = tokenizer.unk_token_id\n","max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n","\n","def tokenize_bert(sentence):\n","    tokens = tokenizer.tokenize(sentence) \n","    return tokens\n","\n","\n","def split_and_cut(sentence):\n","    tokens = sentence.strip().split(\" \")\n","    tokens = tokens[:max_input_length] # make sure that it does not overflow\n","    return tokens"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":234930,"status":"ok","timestamp":1666539821524,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"iQ5zeFtmRWrx"},"outputs":[],"source":["# [SEP]: A special token separating two different sentences in the same input (used by BERT for instance)\n","# [CLS]: A special token representing the class of the input (used by BERT for instance)\n","# They do not contribute to sentence length and will be ignored by attention mask\n","train_df['q1_padded'] = '[CLS] ' + train_df['q1_trimmed'] + ' [SEP] '\n","train_df['q2_padded'] = train_df['q2_trimmed'] + ' [SEP]'\n","\n","train_df['q1_bert_tokens'] = train_df['q1_padded'].apply(lambda x: tokenize_bert(x))\n","train_df['q2_bert_tokens'] = train_df['q2_padded'].apply(lambda x: tokenize_bert(x))\n","\n","val_df['q1_padded'] = '[CLS] ' + val_df['q1_trimmed'] + ' [SEP] '\n","val_df['q2_padded'] = val_df['q2_trimmed'] + ' [SEP]'\n","\n","val_df['q1_bert_tokens'] = val_df['q1_padded'].apply(lambda x: tokenize_bert(x))\n","val_df['q2_bert_tokens'] = val_df['q2_padded'].apply(lambda x: tokenize_bert(x))\n","\n","test_df['q1_padded'] = '[CLS] ' + test_df['q1_trimmed'] + ' [SEP] '\n","test_df['q2_padded'] = test_df['q2_trimmed'] + ' [SEP]'\n","\n","test_df['q1_bert_tokens'] = test_df['q1_padded'].apply(lambda x: tokenize_bert(x))\n","test_df['q2_bert_tokens'] = test_df['q2_padded'].apply(lambda x: tokenize_bert(x))"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":2473,"status":"ok","timestamp":1666539825465,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"QlbbXVysRWrx"},"outputs":[],"source":["train_df['q1_token_type'] = train_df['q1_bert_tokens'].apply(lambda x: sent1_token_type(x))\n","train_df['q2_token_type'] = train_df['q2_bert_tokens'].apply(lambda x: sent2_token_type(x))\n","\n","val_df['q1_token_type'] = val_df['q1_bert_tokens'].apply(lambda x: sent1_token_type(x))\n","val_df['q2_token_type'] = val_df['q2_bert_tokens'].apply(lambda x: sent2_token_type(x))\n","\n","test_df['q1_token_type'] = test_df['q1_bert_tokens'].apply(lambda x: sent1_token_type(x))\n","test_df['q2_token_type'] = test_df['q2_bert_tokens'].apply(lambda x: sent2_token_type(x))"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":4027,"status":"ok","timestamp":1666539829490,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"PfvTpXZhRWry"},"outputs":[],"source":["train_df['sequence'] = train_df['q1_bert_tokens'] + train_df['q2_bert_tokens']\n","train_df['attn_mask'] = train_df['sequence'].apply(lambda x: sent2_token_type(x)) # every word needs attention\n","train_df['token_type'] = train_df['q1_token_type'] + train_df['q2_token_type']\n","\n","val_df['sequence'] = val_df['q1_bert_tokens'] + val_df['q2_bert_tokens']\n","val_df['attn_mask'] = val_df['sequence'].apply(lambda x: sent2_token_type(x)) # every word needs attention\n","val_df['token_type'] = val_df['q1_token_type'] + val_df['q2_token_type']\n","\n","test_df['sequence'] = test_df['q1_bert_tokens'] + test_df['q2_bert_tokens']\n","test_df['attn_mask'] = test_df['sequence'].apply(lambda x: sent2_token_type(x)) # every word needs attention\n","test_df['token_type'] = test_df['q1_token_type'] + test_df['q2_token_type']"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":5853,"status":"ok","timestamp":1666539835341,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"fE9QtVHbRWry"},"outputs":[],"source":["# Make all the inputs to be sequential in string instead of list\n","train_df['sequence'] = train_df['sequence'].apply(lambda x: combine_seq(x))\n","train_df['attn_mask'] = train_df['attn_mask'].apply(lambda x: combine_mask(x))\n","train_df['token_type'] = train_df['token_type'].apply(lambda x: combine_mask(x))\n","\n","val_df['sequence'] = val_df['sequence'].apply(lambda x: combine_seq(x))\n","val_df['attn_mask'] = val_df['attn_mask'].apply(lambda x: combine_mask(x))\n","val_df['token_type'] = val_df['token_type'].apply(lambda x: combine_mask(x))\n","\n","test_df['sequence'] = test_df['sequence'].apply(lambda x: combine_seq(x))\n","test_df['attn_mask'] = test_df['attn_mask'].apply(lambda x: combine_mask(x))\n","test_df['token_type'] = test_df['token_type'].apply(lambda x: combine_mask(x))"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":503,"status":"ok","timestamp":1666539934413,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"XVA7wQpnRWry"},"outputs":[],"source":["#For sequence\n","TEXT = data.Field(batch_first = True,\n","                  use_vocab = False,\n","                  tokenize = split_and_cut,\n","                  preprocessing = tokenizer.convert_tokens_to_ids,\n","                  pad_token = pad_token_idx,\n","                  unk_token = unk_token_idx)\n","#For label\n","LABEL = data.LabelField()\n","\n","#For Attention mask\n","ATTENTION = data.Field(batch_first = True,\n","                  use_vocab = False,\n","                  tokenize = split_and_cut,\n","                  preprocessing = convert_mask,\n","                  pad_token = pad_token_idx)\n","#For token type ids\n","TTYPE = data.Field(batch_first = True,\n","                  use_vocab = False,\n","                  tokenize = split_and_cut,\n","                  preprocessing = convert_mask,\n","                  pad_token = 1)"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":487,"status":"ok","timestamp":1666539937549,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"UVLaQUMfRWry"},"outputs":[],"source":["train_df = train_df[['sequence', 'attn_mask', 'token_type', 'is_duplicate']]\n","val_df = val_df[['sequence', 'attn_mask', 'token_type', 'is_duplicate']]\n","test_df = test_df[['sequence', 'attn_mask', 'token_type', 'is_duplicate']]"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":2654,"status":"ok","timestamp":1666537406959,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"eIy1jRIo_zIv"},"outputs":[],"source":["# train_df = pd.read_csv('/content/drive/MyDrive/duplicate/bert_train.csv')\n","# val_df = pd.read_csv('/content/drive/MyDrive/duplicate/bert_val.csv')\n","# test_df = pd.read_csv('/content/drive/MyDrive/duplicate/bert_test.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8eNyq9gQ_77p"},"outputs":[],"source":["# train_df = train_df.sample(frac=0.3)\n","# val_df = val_df.sample(frac=0.3)\n","# test_df = test_df.sample(frac=0.3)"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":5322,"status":"ok","timestamp":1666540110197,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"tlsaO6e1RWrz"},"outputs":[],"source":["train_df.to_csv('/content/drive/MyDrive/duplicate/bert_train.csv', index = False)\n","val_df.to_csv('/content/drive/MyDrive/duplicate/bert_val.csv', index = False)\n","test_df.to_csv('/content/drive/MyDrive/duplicate/bert_test.csv', index = False)"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22332,"status":"ok","timestamp":1666540134511,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"Iuq-A-gKRWrz","outputId":"98819982-b17b-4f3f-84e4-63f47a4a50cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["fields = [('sequence', TEXT), ('attn_mask', ATTENTION), ('token_type', TTYPE), ('is_duplicate', LABEL)]\n","\n","train_data, valid_data, test_data = data.TabularDataset.splits(path = '/content/drive/MyDrive/duplicate/',\n","                                                    train = 'bert_train.csv',\n","                                                    validation = 'bert_val.csv',\n","                                                    test = 'bert_test.csv',\n","                                                    format = 'csv',\n","                                                    fields = fields,\n","                                                    skip_header = True)\n","#Create iterator\n","BATCH_SIZE = 16\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1042,"status":"ok","timestamp":1666540161981,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"mzK6xog__G6x","outputId":"e5c28e5c-a793-499d-b3c1-59de4cd29e1b"},"outputs":[{"name":"stdout","output_type":"stream","text":["323429 40429 40429\n"]}],"source":["print(len(train_data), len(valid_data), len(test_data))"]},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1666540164203,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"K8W-OB4KRWrz"},"outputs":[],"source":["# Create Python class object of the datasets with predefined batch size (following pytorch format)\n","train_iterator, valid_iterator, test_iterator= data.BucketIterator.splits((train_data, valid_data, test_data), \n","                                                            batch_size = BATCH_SIZE,\n","                                                            sort_key = lambda x: len(x.sequence),\n","                                                            sort_within_batch = False, \n","                                                            device = device)"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":672,"status":"ok","timestamp":1666540167281,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"PMoGe2_XRWrz","outputId":"5a51a239-0d5f-47b3-9ba8-7cf78a35ec52"},"outputs":[{"name":"stdout","output_type":"stream","text":["2\n"]}],"source":["LABEL.build_vocab(train_data)\n","print(len(LABEL.vocab))"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2639,"status":"ok","timestamp":1666540172455,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"eMnXJo1aRWrz","outputId":"15f09678-dc8f-4b50-abc0-3c9dc60846c7"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["bert_model = BertModel.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":486,"status":"ok","timestamp":1666540183276,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"LNPMeeZ8RWrz"},"outputs":[],"source":["class BERTNLIModel(nn.Module):\n","    def __init__(self, bert_model, output_dim):\n","        super().__init__()\n","        self.bert = bert_model\n","        embedding_dim = bert_model.config.to_dict()['hidden_size']\n","        self.out = nn.Linear(embedding_dim, output_dim)\n","        \n","    def forward(self, sequence, attn_mask, token_type):\n","        embedded = self.bert(input_ids=sequence, attention_mask=attn_mask, token_type_ids=token_type)[1]\n","        output = self.out(embedded)\n","        return output\n","\n","OUTPUT_DIM = len(LABEL.vocab)\n","model = BERTNLIModel(bert_model, OUTPUT_DIM).to(device)\n","#model.load_state_dict(torch.load('/content/drive/MyDrive/duplicate/bert_model_full.pt'))"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1666540186945,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"cwjuF0R8RWr0","outputId":"a8140d87-90f6-40fe-fac6-e5a1390fa63f"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}],"source":["optimizer = transformers.AdamW(model.parameters(),lr=2e-5,eps=1e-6,correct_bias=False) # AdamW algorithm as optimizer\n","criterion = nn.CrossEntropyLoss().to(device) # cross-entropy loss as loss function\n","\n","def get_scheduler(optimizer, warmup_steps):\n","    \"\"\"\n","    Create a schedule with a constant learning rate preceded by a warmup period during \n","    which the learning rate increases linearly between 0 and the initial lr set in the optimizer.\n","\n","    Pytorch learning rate scheduler is used to find the optimal learning rate for various models by conisdering the model architecture and parameters\n","    \"\"\"\n","    scheduler = transformers.get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)\n","    return scheduler"]},{"cell_type":"code","execution_count":50,"metadata":{"executionInfo":{"elapsed":513,"status":"ok","timestamp":1666540194412,"user":{"displayName":"Yung Jer Wong","userId":"11346674356050095853"},"user_tz":-480},"id":"HlfIFHxIRWr0"},"outputs":[],"source":["def categorical_accuracy(preds, y):\n","    max_preds = preds.argmax(dim = 1, keepdim = True) # just keep the max argument\n","\n","    correct = (max_preds.squeeze(1)==y).float() # removed all the dimensions of input of size 1\n","\n","    return correct.sum() / len(y)\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n","\n","def train(model, iterator, optimizer, criterion, scheduler):\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    model.train()\n","    for batch in iterator:\n","        optimizer.zero_grad() # clear gradients first\n","        torch.cuda.empty_cache() # releases all unoccupied cached memory\n","        \n","        sequence = batch.sequence\n","        attn_mask = batch.attn_mask\n","        token_type = batch.token_type\n","        label = batch.is_duplicate\n","        \n","        predictions = model(sequence, attn_mask, token_type)\n","        loss = criterion(predictions, label)\n","        acc = categorical_accuracy(predictions, label)\n","        \n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        \n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","def evaluate(model, iterator, criterion):\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for batch in iterator:\n","            sequence = batch.sequence\n","\n","            attn_mask = batch.attn_mask\n","            token_type = batch.token_type\n","            labels = batch.is_duplicate\n","\n","            predictions = model(sequence, attn_mask, token_type)\n","            loss = criterion(predictions, labels)\n","            acc = categorical_accuracy(predictions, labels)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7cnFQzvpRWr0","outputId":"0f6a3335-aecd-4ece-c610-a59e19a04d50"},"outputs":[{"name":"stdout","output_type":"stream","text":["################################# Epoch 1 #################################\n","Epoch: 01 | Epoch Time: 63m 19s\n","Train Loss: 0.368 | Train Acc: 82.35%\n","Val. Loss: 0.278 | Val. Acc: 87.77%\n","################################# Epoch 2 #################################\n","Epoch: 02 | Epoch Time: 63m 17s\n","Train Loss: 0.255 | Train Acc: 89.11%\n","Val. Loss: 0.250 | Val. Acc: 89.41%\n","################################# Epoch 3 #################################\n","Epoch: 03 | Epoch Time: 63m 16s\n","Train Loss: 0.192 | Train Acc: 92.24%\n","Val. Loss: 0.240 | Val. Acc: 90.17%\n","################################# Epoch 4 #################################\n","Epoch: 04 | Epoch Time: 63m 18s\n","Train Loss: 0.133 | Train Acc: 94.88%\n","Val. Loss: 0.266 | Val. Acc: 90.33%\n","################################# Epoch 5 #################################\n"]}],"source":["N_EPOCHS = 10\n","warmup_percent = 0.2\n","total_steps = math.ceil(N_EPOCHS*len(train_data)*1./BATCH_SIZE)\n","warmup_steps = int(total_steps*warmup_percent)\n","scheduler = get_scheduler(optimizer, warmup_steps)\n","best_valid_loss = float('inf')\n","train_loss_list = []\n","train_acc_list= []\n","valid_loss_list = []\n","valid_acc_list= []\n","\n","for epoch in range(N_EPOCHS):\n","    print('################################# Epoch ' + str(epoch + 1) + ' #################################')\n","    start_time = time.time()\n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, scheduler)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","    end_time = time.time()\n","    train_loss_list.append(train_loss)\n","    train_acc_list.append(train_acc)\n","    valid_loss_list.append(valid_loss)\n","    valid_acc_list.append(valid_acc)\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(),  '/content/drive/MyDrive/duplicate/bert_model_full.pt')\n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}%')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aGQtEIVjzU3p"},"outputs":[],"source":["print(train_loss_list, train_acc_list, valid_loss_list, valid_acc_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zDJwhYtmRWr0"},"outputs":[],"source":["model.load_state_dict(torch.load('/content/drive/MyDrive/duplicate/bert_model_full.pt'))\n","valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cbPtjEdlRWr1"},"outputs":[],"source":["def predict_inference(q1, q2, model, device):\n","    model.eval()\n","    \n","    q1 = '[CLS] ' + str(q1) + ' [SEP]'\n","    q2 = str(q2) + ' [SEP]'\n","    \n","    q1_t = tokenize_bert(q1)\n","    q2_t = tokenize_bert(q2)\n","    \n","    q1_type = sent1_token_type(q1_t)\n","    q2_type = sent2_token_type(q2_t)\n","    \n","    indexes = q1_t + q2_t\n","    indexes = tokenizer.convert_tokens_to_ids(indexes)\n","    \n","    indexes_type = q1_type + q2_type\n","    \n","    attn_mask = sent2_token_type(indexes)\n","    \n","    indexes = torch.LongTensor(indexes).unsqueeze(0).to(device)\n","    indexes_type = torch.LongTensor(indexes_type).unsqueeze(0).to(device)\n","    attn_mask = torch.LongTensor(attn_mask).unsqueeze(0).to(device)\n","    \n","    prediction = model(indexes, attn_mask, indexes_type)\n","    # prediction = prediction.argmax(dim=-1).item()\n","    return prediction"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"4773a713a8d9157baa5060563bb30fcc500d9ac4a3c23c64afa2c361e1e3f31c"}}},"nbformat":4,"nbformat_minor":0}

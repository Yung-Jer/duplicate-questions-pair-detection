{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wongy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torchtext.legacy import data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertModel\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "#from transformers import *\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wongy\\OneDrive\\Desktop\\duplicate-questions-pair-detection\n"
     ]
    }
   ],
   "source": [
    "abspath = os.path.abspath('')\n",
    "dname = os.path.dirname(abspath)\n",
    "os.chdir(dname)\n",
    "print(dname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_feather('data/processed/train_dataset.feather')\n",
    "test_df = pd.read_feather('data/processed/test_dataset.feather')\n",
    "val_df = pd.read_feather('data/processed/validation_dataset.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trim_sentence(sent):\n",
    "#     try:\n",
    "#         sent = sent.split()\n",
    "#         sent = sent[:128]\n",
    "#         return \" \".join(sent)\n",
    "#     except:\n",
    "#         return sent\n",
    "\n",
    "# train_df['q1_trimmed'] = train_df['q1_cleaned'].apply(lambda x: trim_sentence(x))\n",
    "# train_df['q2_trimmed'] = train_df['q2_cleaned'].apply(lambda x: trim_sentence(x))\n",
    "\n",
    "# val_df['q1_trimmed'] = val_df['q1_cleaned'].apply(lambda x: trim_sentence(x))\n",
    "# val_df['q2_trimmed'] = val_df['q2_cleaned'].apply(lambda x: trim_sentence(x))\n",
    "\n",
    "# test_df['q1_trimmed'] = test_df['q1_cleaned'].apply(lambda x: trim_sentence(x))\n",
    "# test_df['q2_trimmed'] = test_df['q2_cleaned'].apply(lambda x: trim_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def common_words(row):\n",
    "#     w1 = set(map(lambda word: word.lower().strip(), row['q1_cleaned'].split(\" \")))\n",
    "#     w2 = set(map(lambda word: word.lower().strip(), row['q2_cleaned'].split(\" \")))    \n",
    "#     return len(w1 & w2)\n",
    "    \n",
    "# df['jaccard_dist'] = nltk.jaccard_distance(set(df['q1_cleaned']), set(df['q2_cleaned']))\n",
    "# df['common_words'] = df.apply(common_words, axis=1)\n",
    "# df['common_ratio'] = df.apply(lambda row: row['common_words'] / (len(row['q1_cleaned']) + len(row['q2_cleaned'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking the tokens to feed into BERT\n",
    "def sent1_token_type(sentence):\n",
    "    try:\n",
    "        return [0]* len(sentence)\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "#Get list of 1s\n",
    "def sent2_token_type(sentence):\n",
    "    try:\n",
    "        return [1]* len(sentence)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "#combine from lists\n",
    "def combine_seq(seq):\n",
    "    return \" \".join(seq)\n",
    "\n",
    "#combines from lists of int\n",
    "def combine_mask(mask):\n",
    "    mask = list(map(str, mask))\n",
    "    return \" \".join(mask)\n",
    "\n",
    "#convert attention mask back to list of int\n",
    "def convert_mask(tok_ids):\n",
    "    tok_ids = [int(x) for x in tok_ids]\n",
    "    return tok_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "cls_token = tokenizer.cls_token\n",
    "sep_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "cls_token_idx = tokenizer.cls_token_id\n",
    "sep_token_idx = tokenizer.sep_token_id\n",
    "pad_token_idx = tokenizer.pad_token_id\n",
    "unk_token_idx = tokenizer.unk_token_id\n",
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "\n",
    "def tokenize_bert(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def split_and_cut(sentence):\n",
    "    tokens = sentence.strip().split(\" \")\n",
    "    tokens = tokens[:max_input_length] # make sure that it does not overflow\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['q1_padded'] = '[CLS] ' + train_df['q1_trimmed'] + ' [SEP] '\n",
    "train_df['q2_padded'] = train_df['q2_trimmed'] + ' [SEP]'\n",
    "\n",
    "train_df['q1_bert_tokens'] = train_df['q1_padded'].apply(lambda x: tokenize_bert(x))\n",
    "train_df['q2_bert_tokens'] = train_df['q2_padded'].apply(lambda x: tokenize_bert(x))\n",
    "\n",
    "val_df['q1_padded'] = '[CLS] ' + val_df['q1_trimmed'] + ' [SEP] '\n",
    "val_df['q2_padded'] = val_df['q2_trimmed'] + ' [SEP]'\n",
    "\n",
    "val_df['q1_bert_tokens'] = val_df['q1_padded'].apply(lambda x: tokenize_bert(x))\n",
    "val_df['q2_bert_tokens'] = val_df['q2_padded'].apply(lambda x: tokenize_bert(x))\n",
    "\n",
    "test_df['q1_padded'] = '[CLS] ' + test_df['q1_trimmed'] + ' [SEP] '\n",
    "test_df['q2_padded'] = test_df['q2_trimmed'] + ' [SEP]'\n",
    "\n",
    "test_df['q1_bert_tokens'] = test_df['q1_padded'].apply(lambda x: tokenize_bert(x))\n",
    "test_df['q2_bert_tokens'] = test_df['q2_padded'].apply(lambda x: tokenize_bert(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['q1_token_type'] = train_df['q1_bert_tokens'].apply(lambda x: sent1_token_type(x))\n",
    "train_df['q2_token_type'] = train_df['q2_bert_tokens'].apply(lambda x: sent2_token_type(x))\n",
    "\n",
    "val_df['q1_token_type'] = val_df['q1_bert_tokens'].apply(lambda x: sent1_token_type(x))\n",
    "val_df['q2_token_type'] = val_df['q2_bert_tokens'].apply(lambda x: sent2_token_type(x))\n",
    "\n",
    "test_df['q1_token_type'] = test_df['q1_bert_tokens'].apply(lambda x: sent1_token_type(x))\n",
    "test_df['q2_token_type'] = test_df['q2_bert_tokens'].apply(lambda x: sent2_token_type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sequence'] = train_df['q1_bert_tokens'] + train_df['q2_bert_tokens']\n",
    "train_df['attn_mask'] = train_df['sequence'].apply(lambda x: sent2_token_type(x)) # every word needs attention\n",
    "train_df['token_type'] = train_df['q1_token_type'] + train_df['q2_token_type']\n",
    "\n",
    "val_df['sequence'] = val_df['q1_bert_tokens'] + val_df['q2_bert_tokens']\n",
    "val_df['attn_mask'] = val_df['sequence'].apply(lambda x: sent2_token_type(x)) # every word needs attention\n",
    "val_df['token_type'] = val_df['q1_token_type'] + val_df['q2_token_type']\n",
    "\n",
    "test_df['sequence'] = test_df['q1_bert_tokens'] + test_df['q2_bert_tokens']\n",
    "test_df['attn_mask'] = test_df['sequence'].apply(lambda x: sent2_token_type(x)) # every word needs attention\n",
    "test_df['token_type'] = test_df['q1_token_type'] + test_df['q2_token_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all the inputs to be sequential in string instead of list\n",
    "train_df['sequence'] = train_df['sequence'].apply(lambda x: combine_seq(x))\n",
    "train_df['attn_mask'] = train_df['attn_mask'].apply(lambda x: combine_mask(x))\n",
    "train_df['token_type'] = train_df['token_type'].apply(lambda x: combine_mask(x))\n",
    "\n",
    "val_df['sequence'] = val_df['sequence'].apply(lambda x: combine_seq(x))\n",
    "val_df['attn_mask'] = val_df['attn_mask'].apply(lambda x: combine_mask(x))\n",
    "val_df['token_type'] = val_df['token_type'].apply(lambda x: combine_mask(x))\n",
    "\n",
    "test_df['sequence'] = test_df['sequence'].apply(lambda x: combine_seq(x))\n",
    "test_df['attn_mask'] = test_df['attn_mask'].apply(lambda x: combine_mask(x))\n",
    "test_df['token_type'] = test_df['token_type'].apply(lambda x: combine_mask(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For sequence\n",
    "TEXT = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = split_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx)\n",
    "#For label\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "#For Attention mask\n",
    "ATTENTION = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = split_and_cut,\n",
    "                  preprocessing = convert_mask,\n",
    "                  pad_token = pad_token_idx)\n",
    "#For token type ids\n",
    "TTYPE = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = split_and_cut,\n",
    "                  preprocessing = convert_mask,\n",
    "                  pad_token = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[['sequence', 'attn_mask', 'token_type', 'is_duplicate']]\n",
    "val_df = val_df[['sequence', 'attn_mask', 'token_type', 'is_duplicate']]\n",
    "test_df = test_df[['sequence', 'attn_mask', 'token_type', 'is_duplicate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv('data/processed/bert_train.csv', index = False)\n",
    "# val_df.to_csv('data/processed/bert_val.csv', index = False)\n",
    "# test_df.to_csv('data/processed/bert_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "fields = [('sequence', TEXT), ('attn_mask', ATTENTION), ('token_type', TTYPE), ('is_duplicate', LABEL)]\n",
    "\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(path = 'data/processed/',\n",
    "                                                    train = 'bert_train.csv',\n",
    "                                                    validation = 'bert_val.csv',\n",
    "                                                    test = 'bert_test.csv',\n",
    "                                                    format = 'csv',\n",
    "                                                    fields = fields,\n",
    "                                                    skip_header = True)\n",
    "#Create iterator\n",
    "BATCH_SIZE = 16\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator= data.BucketIterator.splits((train_data, valid_data, test_data), \n",
    "                                                            batch_size = BATCH_SIZE,\n",
    "                                                            sort_key = lambda x: len(x.sequence),\n",
    "                                                            sort_within_batch = False, \n",
    "                                                            device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "LABEL.build_vocab(train_data)\n",
    "print(len(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTNLIModel(nn.Module):\n",
    "    def __init__(self, bert_model, output_dim):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        embedding_dim = bert_model.config.to_dict()['hidden_size']\n",
    "        self.out = nn.Linear(embedding_dim, output_dim)\n",
    "        \n",
    "    def forward(self, sequence, attn_mask, token_type):\n",
    "        embedded = self.bert(input_ids=sequence, attention_mask=attn_mask, token_type_ids=token_type)[1]\n",
    "        output = self.out(embedded)\n",
    "        return output\n",
    "\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "model = BERTNLIModel(bert_model, OUTPUT_DIM).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wongy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = transformers.AdamW(model.parameters(),lr=2e-5,eps=1e-6,correct_bias=False)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "def get_scheduler(optimizer, warmup_steps):\n",
    "    scheduler = transformers.get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True)\n",
    "\n",
    "    correct = (max_preds.squeeze(1)==y).float()\n",
    "\n",
    "    return correct.sum() / len(y)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, scheduler):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad() # clear gradients first\n",
    "        torch.cuda.empty_cache() # releases all unoccupied cached memory\n",
    "        \n",
    "        sequence = batch.sequence\n",
    "        attn_mask = batch.attn_mask\n",
    "        token_type = batch.token_type\n",
    "        label = batch.is_duplicate\n",
    "        \n",
    "        predictions = model(sequence, attn_mask, token_type)\n",
    "        loss = criterion(predictions, label)\n",
    "        acc = categorical_accuracy(predictions, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            sequence = batch.sequence\n",
    "\n",
    "            attn_mask = batch.attn_mask\n",
    "            token_type = batch.token_type\n",
    "            labels = batch.is_duplicate\n",
    "\n",
    "            predictions = model(sequence, attn_mask, token_type)\n",
    "            loss = criterion(predictions, labels)\n",
    "            acc = categorical_accuracy(predictions, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323429"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [25], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N_EPOCHS):\n\u001b[0;32m      9\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 10\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_iterator, optimizer, criterion, scheduler)\n\u001b[0;32m     11\u001b[0m     valid_loss, valid_acc \u001b[39m=\u001b[39m evaluate(model, valid_iterator, criterion)\n\u001b[0;32m     12\u001b[0m     end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn [24], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion, scheduler)\u001b[0m\n\u001b[0;32m     28\u001b[0m loss \u001b[39m=\u001b[39m criterion(predictions, label)\n\u001b[0;32m     29\u001b[0m acc \u001b[39m=\u001b[39m categorical_accuracy(predictions, label)\n\u001b[1;32m---> 31\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     32\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     33\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\wongy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    247\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    248\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    249\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    254\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 255\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\wongy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\autograd\\__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m--> 147\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[0;32m    148\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    149\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "warmup_percent = 0.2\n",
    "total_steps = math.ceil(N_EPOCHS*len(train_data)*1./BATCH_SIZE)\n",
    "warmup_steps = int(total_steps*warmup_percent)\n",
    "scheduler = get_scheduler(optimizer, warmup_steps)\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, scheduler)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'bert_based_quora_model.pt')\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'src/bert_based_quora_model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39msrc/bert_based_quora_model.pt\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m      2\u001b[0m val_loss, val_acc \u001b[39m=\u001b[39m evaluate(model, valid_iterator, criterion)\n",
      "File \u001b[1;32mc:\\Users\\wongy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\serialization.py:594\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    592\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 594\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    595\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    596\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    598\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    599\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\wongy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 230\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    231\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\wongy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 211\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'src/bert_based_quora_model.pt'"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('src/bert_based_quora_model.pt'))\n",
    "val_loss, val_acc = evaluate(model, valid_iterator, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'bert_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_inference(q1, q2, model, device):\n",
    "    model.eval()\n",
    "    \n",
    "    q1 = '[CLS] ' + str(q1) + ' [SEP]'\n",
    "    q2 = str(q2) + ' [SEP]'\n",
    "    \n",
    "    q1_t = tokenize_bert(q1)\n",
    "    q2_t = tokenize_bert(q2)\n",
    "    \n",
    "    q1_type = sent1_token_type(q1_t)\n",
    "    q2_type = sent2_token_type(q2_t)\n",
    "    \n",
    "    indexes = q1_t + q2_t\n",
    "    indexes = tokenizer.convert_tokens_to_ids(indexes)\n",
    "    \n",
    "    indexes_type = q1_type + q2_type\n",
    "    \n",
    "    attn_mask = sent2_token_type(indexes)\n",
    "    \n",
    "    indexes = torch.LongTensor(indexes).unsqueeze(0).to(device)\n",
    "    indexes_type = torch.LongTensor(indexes_type).unsqueeze(0).to(device)\n",
    "    attn_mask = torch.LongTensor(attn_mask).unsqueeze(0).to(device)\n",
    "    \n",
    "    prediction = model(indexes, attn_mask, indexes_type)\n",
    "    # prediction = prediction.argmax(dim=-1).item()\n",
    "    return prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "4773a713a8d9157baa5060563bb30fcc500d9ac4a3c23c64afa2c361e1e3f31c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

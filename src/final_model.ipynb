{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b0e4620",
   "metadata": {},
   "source": [
    "## 1. Import Module & Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1faace61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss, precision_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from itertools import combinations\n",
    "\n",
    "train = pd.read_csv('final_model_train_input.csv')\n",
    "test = pd.read_csv('final_model_test_input.csv')\n",
    "X_train = train.drop(['is_duplicate'],  axis=1)\n",
    "y_train = train['is_duplicate']\n",
    "\n",
    "X_test = test.drop(['is_duplicate'],  axis=1)\n",
    "y_test = test['is_duplicate']\n",
    "\n",
    "model_list = X_train.columns.tolist()\n",
    "input_list = []\n",
    "for i in range(2, len(model_list)):\n",
    "    input_list += [\",\".join(map(str, comb)) for comb in combinations(model_list, i)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac4ba7e",
   "metadata": {},
   "source": [
    "# 2. Stepwise Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68b21635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stacked Model</th>\n",
       "      <th>Train Log Loss</th>\n",
       "      <th>Train Precision</th>\n",
       "      <th>Test Log Loss</th>\n",
       "      <th>Test Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert,lgbm</td>\n",
       "      <td>0.125318</td>\n",
       "      <td>0.935907</td>\n",
       "      <td>0.133022</td>\n",
       "      <td>0.930700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert,lgbm,mlp</td>\n",
       "      <td>0.125301</td>\n",
       "      <td>0.935968</td>\n",
       "      <td>0.133088</td>\n",
       "      <td>0.930564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert,mlp</td>\n",
       "      <td>0.133120</td>\n",
       "      <td>0.930157</td>\n",
       "      <td>0.135653</td>\n",
       "      <td>0.927189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lgbm,mlp</td>\n",
       "      <td>0.131216</td>\n",
       "      <td>0.936443</td>\n",
       "      <td>0.144879</td>\n",
       "      <td>0.927349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bert,xgboost</td>\n",
       "      <td>0.099880</td>\n",
       "      <td>0.946260</td>\n",
       "      <td>0.172028</td>\n",
       "      <td>0.934690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lgbm,xgboost</td>\n",
       "      <td>0.120456</td>\n",
       "      <td>0.937038</td>\n",
       "      <td>0.180276</td>\n",
       "      <td>0.921289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bert,lgbm,xgboost</td>\n",
       "      <td>0.099365</td>\n",
       "      <td>0.946301</td>\n",
       "      <td>0.180886</td>\n",
       "      <td>0.934214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bert,rf</td>\n",
       "      <td>0.075601</td>\n",
       "      <td>0.959547</td>\n",
       "      <td>0.219718</td>\n",
       "      <td>0.894265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bert,rf,xgboost</td>\n",
       "      <td>0.074951</td>\n",
       "      <td>0.960062</td>\n",
       "      <td>0.225169</td>\n",
       "      <td>0.886706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bert,lgbm,rf</td>\n",
       "      <td>0.070369</td>\n",
       "      <td>0.963516</td>\n",
       "      <td>0.250165</td>\n",
       "      <td>0.880541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bert,lgbm,rf,xgboost</td>\n",
       "      <td>0.070287</td>\n",
       "      <td>0.963610</td>\n",
       "      <td>0.250928</td>\n",
       "      <td>0.877893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lgbm,rf</td>\n",
       "      <td>0.098333</td>\n",
       "      <td>0.948006</td>\n",
       "      <td>0.257836</td>\n",
       "      <td>0.872240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lgbm,rf,xgboost</td>\n",
       "      <td>0.096917</td>\n",
       "      <td>0.949378</td>\n",
       "      <td>0.265358</td>\n",
       "      <td>0.862398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>siam_lstm,bert,rf</td>\n",
       "      <td>0.060116</td>\n",
       "      <td>0.968002</td>\n",
       "      <td>0.339164</td>\n",
       "      <td>0.857079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>siam_lstm,bert,rf,xgboost</td>\n",
       "      <td>0.058736</td>\n",
       "      <td>0.969207</td>\n",
       "      <td>0.354611</td>\n",
       "      <td>0.839498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lgbm,mlp,xgboost</td>\n",
       "      <td>0.080695</td>\n",
       "      <td>0.960053</td>\n",
       "      <td>0.376436</td>\n",
       "      <td>0.910260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bert,lgbm,mlp,xgboost</td>\n",
       "      <td>0.073597</td>\n",
       "      <td>0.961810</td>\n",
       "      <td>0.379008</td>\n",
       "      <td>0.917727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bert,mlp,rf</td>\n",
       "      <td>0.022105</td>\n",
       "      <td>0.989267</td>\n",
       "      <td>0.385667</td>\n",
       "      <td>0.845967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bert,lgbm,mlp,rf</td>\n",
       "      <td>0.022087</td>\n",
       "      <td>0.989258</td>\n",
       "      <td>0.386913</td>\n",
       "      <td>0.845587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bert,mlp,xgboost</td>\n",
       "      <td>0.074098</td>\n",
       "      <td>0.961255</td>\n",
       "      <td>0.392708</td>\n",
       "      <td>0.915825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>bert,mlp,rf,xgboost</td>\n",
       "      <td>0.019688</td>\n",
       "      <td>0.990340</td>\n",
       "      <td>0.418745</td>\n",
       "      <td>0.864788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bert,lgbm,mlp,rf,xgboost</td>\n",
       "      <td>0.019551</td>\n",
       "      <td>0.990357</td>\n",
       "      <td>0.425052</td>\n",
       "      <td>0.863459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>lgbm,mlp,rf</td>\n",
       "      <td>0.025918</td>\n",
       "      <td>0.987067</td>\n",
       "      <td>0.431145</td>\n",
       "      <td>0.833229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>siam_lstm,bert,xgboost</td>\n",
       "      <td>0.072229</td>\n",
       "      <td>0.961337</td>\n",
       "      <td>0.451558</td>\n",
       "      <td>0.816917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>lgbm,mlp,rf,xgboost</td>\n",
       "      <td>0.023345</td>\n",
       "      <td>0.988495</td>\n",
       "      <td>0.468120</td>\n",
       "      <td>0.847799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>siam_lstm,lgbm,rf</td>\n",
       "      <td>0.073424</td>\n",
       "      <td>0.961512</td>\n",
       "      <td>0.468976</td>\n",
       "      <td>0.790281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rf,xgboost</td>\n",
       "      <td>0.126048</td>\n",
       "      <td>0.934638</td>\n",
       "      <td>0.474674</td>\n",
       "      <td>0.770072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>siam_lstm,bert,lgbm,rf</td>\n",
       "      <td>0.052182</td>\n",
       "      <td>0.973034</td>\n",
       "      <td>0.475141</td>\n",
       "      <td>0.812119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>siam_lstm,bert,lgbm,rf,xgboost</td>\n",
       "      <td>0.051830</td>\n",
       "      <td>0.973429</td>\n",
       "      <td>0.476347</td>\n",
       "      <td>0.804882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>siam_lstm,lgbm,rf,xgboost</td>\n",
       "      <td>0.071481</td>\n",
       "      <td>0.963107</td>\n",
       "      <td>0.481174</td>\n",
       "      <td>0.776057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>siam_lstm,lgbm,xgboost</td>\n",
       "      <td>0.085599</td>\n",
       "      <td>0.955254</td>\n",
       "      <td>0.522937</td>\n",
       "      <td>0.673115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>siam_lstm,bert,mlp,rf</td>\n",
       "      <td>0.016154</td>\n",
       "      <td>0.992188</td>\n",
       "      <td>0.534114</td>\n",
       "      <td>0.810971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>siam_lstm,bert,mlp,rf,xgboost</td>\n",
       "      <td>0.015042</td>\n",
       "      <td>0.992511</td>\n",
       "      <td>0.556099</td>\n",
       "      <td>0.826416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>siam_lstm,bert,lgbm,mlp,rf</td>\n",
       "      <td>0.015717</td>\n",
       "      <td>0.992572</td>\n",
       "      <td>0.572737</td>\n",
       "      <td>0.800152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>siam_lstm,lgbm</td>\n",
       "      <td>0.088885</td>\n",
       "      <td>0.954395</td>\n",
       "      <td>0.595909</td>\n",
       "      <td>0.604472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>siam_lstm,lgbm,mlp,rf</td>\n",
       "      <td>0.018619</td>\n",
       "      <td>0.990975</td>\n",
       "      <td>0.611875</td>\n",
       "      <td>0.784659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>siam_lstm,bert,lgbm,xgboost</td>\n",
       "      <td>0.068893</td>\n",
       "      <td>0.962644</td>\n",
       "      <td>0.614945</td>\n",
       "      <td>0.739263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>siam_lstm,lgbm,mlp,rf,xgboost</td>\n",
       "      <td>0.017348</td>\n",
       "      <td>0.991397</td>\n",
       "      <td>0.639159</td>\n",
       "      <td>0.799904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>mlp,rf</td>\n",
       "      <td>0.037342</td>\n",
       "      <td>0.981237</td>\n",
       "      <td>0.710639</td>\n",
       "      <td>0.746463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>siam_lstm,bert</td>\n",
       "      <td>0.078303</td>\n",
       "      <td>0.958047</td>\n",
       "      <td>0.713591</td>\n",
       "      <td>0.530231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>siam_lstm,rf</td>\n",
       "      <td>0.078695</td>\n",
       "      <td>0.959117</td>\n",
       "      <td>0.728211</td>\n",
       "      <td>0.696648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>mlp,rf,xgboost</td>\n",
       "      <td>0.031954</td>\n",
       "      <td>0.983736</td>\n",
       "      <td>0.738627</td>\n",
       "      <td>0.771033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>siam_lstm,rf,xgboost</td>\n",
       "      <td>0.077226</td>\n",
       "      <td>0.960524</td>\n",
       "      <td>0.752626</td>\n",
       "      <td>0.682925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>siam_lstm,bert,lgbm</td>\n",
       "      <td>0.078187</td>\n",
       "      <td>0.957966</td>\n",
       "      <td>0.768858</td>\n",
       "      <td>0.519122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>siam_lstm,lgbm,mlp</td>\n",
       "      <td>0.081837</td>\n",
       "      <td>0.959360</td>\n",
       "      <td>0.822427</td>\n",
       "      <td>0.515197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>mlp,xgboost</td>\n",
       "      <td>0.155317</td>\n",
       "      <td>0.919136</td>\n",
       "      <td>0.828669</td>\n",
       "      <td>0.784664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>siam_lstm,lgbm,mlp,xgboost</td>\n",
       "      <td>0.055038</td>\n",
       "      <td>0.972455</td>\n",
       "      <td>0.865073</td>\n",
       "      <td>0.625104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>siam_lstm,bert,mlp,xgboost</td>\n",
       "      <td>0.049707</td>\n",
       "      <td>0.974287</td>\n",
       "      <td>0.874487</td>\n",
       "      <td>0.612542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>siam_lstm,bert,lgbm,mlp</td>\n",
       "      <td>0.076419</td>\n",
       "      <td>0.959716</td>\n",
       "      <td>0.889959</td>\n",
       "      <td>0.498171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>siam_lstm,mlp,rf</td>\n",
       "      <td>0.021181</td>\n",
       "      <td>0.990138</td>\n",
       "      <td>0.902872</td>\n",
       "      <td>0.701677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>siam_lstm,mlp,rf,xgboost</td>\n",
       "      <td>0.019426</td>\n",
       "      <td>0.990598</td>\n",
       "      <td>0.910703</td>\n",
       "      <td>0.725168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>siam_lstm,bert,lgbm,mlp,xgboost</td>\n",
       "      <td>0.049526</td>\n",
       "      <td>0.974156</td>\n",
       "      <td>0.918478</td>\n",
       "      <td>0.596117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>siam_lstm,bert,mlp</td>\n",
       "      <td>0.076609</td>\n",
       "      <td>0.959551</td>\n",
       "      <td>0.940960</td>\n",
       "      <td>0.490638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>siam_lstm,xgboost</td>\n",
       "      <td>0.100557</td>\n",
       "      <td>0.947504</td>\n",
       "      <td>1.180005</td>\n",
       "      <td>0.453171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>siam_lstm,mlp,xgboost</td>\n",
       "      <td>0.071521</td>\n",
       "      <td>0.963843</td>\n",
       "      <td>1.747320</td>\n",
       "      <td>0.401677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>siam_lstm,mlp</td>\n",
       "      <td>0.118757</td>\n",
       "      <td>0.941768</td>\n",
       "      <td>2.045645</td>\n",
       "      <td>0.367474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Stacked Model  Train Log Loss  Train Precision  \\\n",
       "0                         bert,lgbm        0.125318         0.935907   \n",
       "1                     bert,lgbm,mlp        0.125301         0.935968   \n",
       "2                          bert,mlp        0.133120         0.930157   \n",
       "3                          lgbm,mlp        0.131216         0.936443   \n",
       "4                      bert,xgboost        0.099880         0.946260   \n",
       "5                      lgbm,xgboost        0.120456         0.937038   \n",
       "6                 bert,lgbm,xgboost        0.099365         0.946301   \n",
       "7                           bert,rf        0.075601         0.959547   \n",
       "8                   bert,rf,xgboost        0.074951         0.960062   \n",
       "9                      bert,lgbm,rf        0.070369         0.963516   \n",
       "10             bert,lgbm,rf,xgboost        0.070287         0.963610   \n",
       "11                          lgbm,rf        0.098333         0.948006   \n",
       "12                  lgbm,rf,xgboost        0.096917         0.949378   \n",
       "13                siam_lstm,bert,rf        0.060116         0.968002   \n",
       "14        siam_lstm,bert,rf,xgboost        0.058736         0.969207   \n",
       "15                 lgbm,mlp,xgboost        0.080695         0.960053   \n",
       "16            bert,lgbm,mlp,xgboost        0.073597         0.961810   \n",
       "17                      bert,mlp,rf        0.022105         0.989267   \n",
       "18                 bert,lgbm,mlp,rf        0.022087         0.989258   \n",
       "19                 bert,mlp,xgboost        0.074098         0.961255   \n",
       "20              bert,mlp,rf,xgboost        0.019688         0.990340   \n",
       "21         bert,lgbm,mlp,rf,xgboost        0.019551         0.990357   \n",
       "22                      lgbm,mlp,rf        0.025918         0.987067   \n",
       "23           siam_lstm,bert,xgboost        0.072229         0.961337   \n",
       "24              lgbm,mlp,rf,xgboost        0.023345         0.988495   \n",
       "25                siam_lstm,lgbm,rf        0.073424         0.961512   \n",
       "26                       rf,xgboost        0.126048         0.934638   \n",
       "27           siam_lstm,bert,lgbm,rf        0.052182         0.973034   \n",
       "28   siam_lstm,bert,lgbm,rf,xgboost        0.051830         0.973429   \n",
       "29        siam_lstm,lgbm,rf,xgboost        0.071481         0.963107   \n",
       "30           siam_lstm,lgbm,xgboost        0.085599         0.955254   \n",
       "31            siam_lstm,bert,mlp,rf        0.016154         0.992188   \n",
       "32    siam_lstm,bert,mlp,rf,xgboost        0.015042         0.992511   \n",
       "33       siam_lstm,bert,lgbm,mlp,rf        0.015717         0.992572   \n",
       "34                   siam_lstm,lgbm        0.088885         0.954395   \n",
       "35            siam_lstm,lgbm,mlp,rf        0.018619         0.990975   \n",
       "36      siam_lstm,bert,lgbm,xgboost        0.068893         0.962644   \n",
       "37    siam_lstm,lgbm,mlp,rf,xgboost        0.017348         0.991397   \n",
       "38                           mlp,rf        0.037342         0.981237   \n",
       "39                   siam_lstm,bert        0.078303         0.958047   \n",
       "40                     siam_lstm,rf        0.078695         0.959117   \n",
       "41                   mlp,rf,xgboost        0.031954         0.983736   \n",
       "42             siam_lstm,rf,xgboost        0.077226         0.960524   \n",
       "43              siam_lstm,bert,lgbm        0.078187         0.957966   \n",
       "44               siam_lstm,lgbm,mlp        0.081837         0.959360   \n",
       "45                      mlp,xgboost        0.155317         0.919136   \n",
       "46       siam_lstm,lgbm,mlp,xgboost        0.055038         0.972455   \n",
       "47       siam_lstm,bert,mlp,xgboost        0.049707         0.974287   \n",
       "48          siam_lstm,bert,lgbm,mlp        0.076419         0.959716   \n",
       "49                 siam_lstm,mlp,rf        0.021181         0.990138   \n",
       "50         siam_lstm,mlp,rf,xgboost        0.019426         0.990598   \n",
       "51  siam_lstm,bert,lgbm,mlp,xgboost        0.049526         0.974156   \n",
       "52               siam_lstm,bert,mlp        0.076609         0.959551   \n",
       "53                siam_lstm,xgboost        0.100557         0.947504   \n",
       "54            siam_lstm,mlp,xgboost        0.071521         0.963843   \n",
       "55                    siam_lstm,mlp        0.118757         0.941768   \n",
       "\n",
       "    Test Log Loss  Test Precision  \n",
       "0        0.133022        0.930700  \n",
       "1        0.133088        0.930564  \n",
       "2        0.135653        0.927189  \n",
       "3        0.144879        0.927349  \n",
       "4        0.172028        0.934690  \n",
       "5        0.180276        0.921289  \n",
       "6        0.180886        0.934214  \n",
       "7        0.219718        0.894265  \n",
       "8        0.225169        0.886706  \n",
       "9        0.250165        0.880541  \n",
       "10       0.250928        0.877893  \n",
       "11       0.257836        0.872240  \n",
       "12       0.265358        0.862398  \n",
       "13       0.339164        0.857079  \n",
       "14       0.354611        0.839498  \n",
       "15       0.376436        0.910260  \n",
       "16       0.379008        0.917727  \n",
       "17       0.385667        0.845967  \n",
       "18       0.386913        0.845587  \n",
       "19       0.392708        0.915825  \n",
       "20       0.418745        0.864788  \n",
       "21       0.425052        0.863459  \n",
       "22       0.431145        0.833229  \n",
       "23       0.451558        0.816917  \n",
       "24       0.468120        0.847799  \n",
       "25       0.468976        0.790281  \n",
       "26       0.474674        0.770072  \n",
       "27       0.475141        0.812119  \n",
       "28       0.476347        0.804882  \n",
       "29       0.481174        0.776057  \n",
       "30       0.522937        0.673115  \n",
       "31       0.534114        0.810971  \n",
       "32       0.556099        0.826416  \n",
       "33       0.572737        0.800152  \n",
       "34       0.595909        0.604472  \n",
       "35       0.611875        0.784659  \n",
       "36       0.614945        0.739263  \n",
       "37       0.639159        0.799904  \n",
       "38       0.710639        0.746463  \n",
       "39       0.713591        0.530231  \n",
       "40       0.728211        0.696648  \n",
       "41       0.738627        0.771033  \n",
       "42       0.752626        0.682925  \n",
       "43       0.768858        0.519122  \n",
       "44       0.822427        0.515197  \n",
       "45       0.828669        0.784664  \n",
       "46       0.865073        0.625104  \n",
       "47       0.874487        0.612542  \n",
       "48       0.889959        0.498171  \n",
       "49       0.902872        0.701677  \n",
       "50       0.910703        0.725168  \n",
       "51       0.918478        0.596117  \n",
       "52       0.940960        0.490638  \n",
       "53       1.180005        0.453171  \n",
       "54       1.747320        0.401677  \n",
       "55       2.045645        0.367474  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result_df = pd.Dataframe(columns = ['Input', 'Train Log Loss', 'Trai Precision', 'Test Log Loss', 'Test Precision'])\n",
    "result_list = []\n",
    "log_clf = LogisticRegression()\n",
    "for i in input_list:\n",
    "    subset = i.split(\",\")\n",
    "    train_subset = X_train[subset]\n",
    "    test_subset = X_test[subset]\n",
    "    log_clf.fit(train_subset,y_train)\n",
    "    preds_train = log_clf.predict(train_subset)\n",
    "    preds_prob_train = log_clf.predict_proba(train_subset)\n",
    "    preds_test = log_clf.predict(test_subset)\n",
    "    preds_prob_test = log_clf.predict_proba(test_subset)\n",
    "    l = [i, log_loss(y_train, preds_prob_train),precision_score(y_train, preds_train), \n",
    "                       log_loss(y_test, preds_prob_test), precision_score(y_test, preds_test)]\n",
    "    result_list.append(l)\n",
    "\n",
    "result_df = pd.DataFrame(result_list)\n",
    "result_df.columns = ['Stacked Model','Train Log Loss', 'Train Precision', 'Test Log Loss', 'Test Precision']\n",
    "result_df = result_df.sort_values(by =['Test Log Loss'], ascending = True)\n",
    "result_df.reset_index(drop = True, inplace = True)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c8a7a",
   "metadata": {},
   "source": [
    "# 3. Best Subset Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54e6183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_subset_feature = result_df['Stacked Model'].values[0].split(\",\")\n",
    "X_train = X_train[best_subset_feature]\n",
    "X_test = X_test[best_subset_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d4f318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf.fit(X_train, y_train)\n",
    "preds_train = log_clf.predict(X_train)\n",
    "preds_prob_train = log_clf.predict_proba(X_train)\n",
    "preds_test = log_clf.predict(X_test)\n",
    "preds_prob_test = log_clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6203eaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train log loss is: 0.12531842132839877\n",
      "The train precision is: 0.9359066709721222\n",
      "The test log loss is: 0.13302161760440695\n",
      "The test precision is: 0.9307002505604642\n"
     ]
    }
   ],
   "source": [
    "print(\"The train log loss is:\", log_loss(y_train, preds_prob_train))\n",
    "print(\"The train precision is:\", precision_score(y_train, preds_train))\n",
    "print(\"The test log loss is:\", log_loss(y_test, preds_prob_test))\n",
    "print(\"The test precision is:\", precision_score(y_test, preds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84de69d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

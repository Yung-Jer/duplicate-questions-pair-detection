{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b0e4620",
   "metadata": {},
   "source": [
    "## 1. Import Module & Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1faace61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss, precision_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from itertools import combinations\n",
    "\n",
    "train = pd.read_csv('../data/processed/final_model_train_input.csv')\n",
    "test = pd.read_csv('../data/processed/final_model_test_input.csv')\n",
    "X_train = train.drop(['is_duplicate'],  axis=1)\n",
    "y_train = train['is_duplicate']\n",
    "\n",
    "X_test = test.drop(['is_duplicate'],  axis=1)\n",
    "y_test = test['is_duplicate']\n",
    "\n",
    "model_list = X_train.columns.tolist()\n",
    "input_list = []\n",
    "for i in range(2, len(model_list)):\n",
    "    input_list += [\",\".join(map(str, comb)) for comb in combinations(model_list, i)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac4ba7e",
   "metadata": {},
   "source": [
    "# 2. Stepwise Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68b21635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stacked Model</th>\n",
       "      <th>Train Log Loss</th>\n",
       "      <th>Train Precision</th>\n",
       "      <th>Test Log Loss</th>\n",
       "      <th>Test Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert,lgbm</td>\n",
       "      <td>0.125318</td>\n",
       "      <td>0.935907</td>\n",
       "      <td>0.133022</td>\n",
       "      <td>0.930700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert,lgbm,mlp</td>\n",
       "      <td>0.125301</td>\n",
       "      <td>0.935968</td>\n",
       "      <td>0.133088</td>\n",
       "      <td>0.930564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert,mlp</td>\n",
       "      <td>0.133120</td>\n",
       "      <td>0.930157</td>\n",
       "      <td>0.135653</td>\n",
       "      <td>0.927189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>siam_lstm,bert</td>\n",
       "      <td>0.089873</td>\n",
       "      <td>0.952187</td>\n",
       "      <td>0.143896</td>\n",
       "      <td>0.928853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>siam_lstm,bert,lgbm</td>\n",
       "      <td>0.089812</td>\n",
       "      <td>0.952045</td>\n",
       "      <td>0.144577</td>\n",
       "      <td>0.928110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lgbm,mlp</td>\n",
       "      <td>0.131216</td>\n",
       "      <td>0.936443</td>\n",
       "      <td>0.144879</td>\n",
       "      <td>0.927349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>siam_lstm,bert,lgbm,mlp</td>\n",
       "      <td>0.087988</td>\n",
       "      <td>0.954055</td>\n",
       "      <td>0.145437</td>\n",
       "      <td>0.928533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>siam_lstm,bert,mlp</td>\n",
       "      <td>0.088223</td>\n",
       "      <td>0.953479</td>\n",
       "      <td>0.146651</td>\n",
       "      <td>0.927930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>siam_lstm,bert,xgboost</td>\n",
       "      <td>0.081837</td>\n",
       "      <td>0.955377</td>\n",
       "      <td>0.157603</td>\n",
       "      <td>0.935726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>siam_lstm,lgbm,mlp</td>\n",
       "      <td>0.094769</td>\n",
       "      <td>0.953671</td>\n",
       "      <td>0.159301</td>\n",
       "      <td>0.922521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>siam_lstm,lgbm</td>\n",
       "      <td>0.102490</td>\n",
       "      <td>0.948372</td>\n",
       "      <td>0.164741</td>\n",
       "      <td>0.916412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bert,xgboost</td>\n",
       "      <td>0.099880</td>\n",
       "      <td>0.946260</td>\n",
       "      <td>0.172028</td>\n",
       "      <td>0.934690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siam_lstm,bert,lgbm,xgboost</td>\n",
       "      <td>0.078409</td>\n",
       "      <td>0.957051</td>\n",
       "      <td>0.173858</td>\n",
       "      <td>0.934537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lgbm,xgboost</td>\n",
       "      <td>0.120456</td>\n",
       "      <td>0.937038</td>\n",
       "      <td>0.180276</td>\n",
       "      <td>0.921289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bert,lgbm,xgboost</td>\n",
       "      <td>0.099365</td>\n",
       "      <td>0.946301</td>\n",
       "      <td>0.180886</td>\n",
       "      <td>0.934214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>siam_lstm,lgbm,xgboost</td>\n",
       "      <td>0.098293</td>\n",
       "      <td>0.949370</td>\n",
       "      <td>0.182577</td>\n",
       "      <td>0.917744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>siam_lstm,bert,rf</td>\n",
       "      <td>0.067014</td>\n",
       "      <td>0.964584</td>\n",
       "      <td>0.202771</td>\n",
       "      <td>0.905172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>siam_lstm,bert,rf,xgboost</td>\n",
       "      <td>0.065609</td>\n",
       "      <td>0.965710</td>\n",
       "      <td>0.211773</td>\n",
       "      <td>0.892403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bert,rf</td>\n",
       "      <td>0.075601</td>\n",
       "      <td>0.959547</td>\n",
       "      <td>0.219718</td>\n",
       "      <td>0.894265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bert,rf,xgboost</td>\n",
       "      <td>0.074951</td>\n",
       "      <td>0.960062</td>\n",
       "      <td>0.225169</td>\n",
       "      <td>0.886706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>siam_lstm,bert,lgbm,rf</td>\n",
       "      <td>0.058722</td>\n",
       "      <td>0.969229</td>\n",
       "      <td>0.232920</td>\n",
       "      <td>0.893046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>siam_lstm,bert,lgbm,rf,xgboost</td>\n",
       "      <td>0.058354</td>\n",
       "      <td>0.969742</td>\n",
       "      <td>0.235411</td>\n",
       "      <td>0.887339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>siam_lstm,lgbm,rf</td>\n",
       "      <td>0.083681</td>\n",
       "      <td>0.956582</td>\n",
       "      <td>0.250067</td>\n",
       "      <td>0.876565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bert,lgbm,rf</td>\n",
       "      <td>0.070369</td>\n",
       "      <td>0.963516</td>\n",
       "      <td>0.250165</td>\n",
       "      <td>0.880541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bert,lgbm,rf,xgboost</td>\n",
       "      <td>0.070287</td>\n",
       "      <td>0.963610</td>\n",
       "      <td>0.250928</td>\n",
       "      <td>0.877893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>lgbm,rf</td>\n",
       "      <td>0.098333</td>\n",
       "      <td>0.948006</td>\n",
       "      <td>0.257836</td>\n",
       "      <td>0.872240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>siam_lstm,lgbm,rf,xgboost</td>\n",
       "      <td>0.081482</td>\n",
       "      <td>0.958344</td>\n",
       "      <td>0.261450</td>\n",
       "      <td>0.864046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>lgbm,rf,xgboost</td>\n",
       "      <td>0.096917</td>\n",
       "      <td>0.949378</td>\n",
       "      <td>0.265358</td>\n",
       "      <td>0.862398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>siam_lstm,mlp</td>\n",
       "      <td>0.146865</td>\n",
       "      <td>0.927355</td>\n",
       "      <td>0.293724</td>\n",
       "      <td>0.849593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>siam_lstm,xgboost</td>\n",
       "      <td>0.121196</td>\n",
       "      <td>0.937322</td>\n",
       "      <td>0.299555</td>\n",
       "      <td>0.873506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>siam_lstm,bert,mlp,xgboost</td>\n",
       "      <td>0.056291</td>\n",
       "      <td>0.970722</td>\n",
       "      <td>0.317810</td>\n",
       "      <td>0.929912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>siam_lstm,bert,lgbm,mlp,xgboost</td>\n",
       "      <td>0.056095</td>\n",
       "      <td>0.970632</td>\n",
       "      <td>0.324254</td>\n",
       "      <td>0.929823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>siam_lstm,lgbm,mlp,xgboost</td>\n",
       "      <td>0.062963</td>\n",
       "      <td>0.968668</td>\n",
       "      <td>0.326607</td>\n",
       "      <td>0.918919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>siam_lstm,bert,mlp,rf</td>\n",
       "      <td>0.017788</td>\n",
       "      <td>0.991536</td>\n",
       "      <td>0.346760</td>\n",
       "      <td>0.862448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>siam_lstm,rf</td>\n",
       "      <td>0.092429</td>\n",
       "      <td>0.952690</td>\n",
       "      <td>0.349740</td>\n",
       "      <td>0.829694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>siam_lstm,bert,lgbm,mlp,rf</td>\n",
       "      <td>0.017344</td>\n",
       "      <td>0.991769</td>\n",
       "      <td>0.351390</td>\n",
       "      <td>0.860478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>siam_lstm,rf,xgboost</td>\n",
       "      <td>0.090736</td>\n",
       "      <td>0.954233</td>\n",
       "      <td>0.364152</td>\n",
       "      <td>0.816474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>siam_lstm,bert,mlp,rf,xgboost</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.991910</td>\n",
       "      <td>0.369391</td>\n",
       "      <td>0.878779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>lgbm,mlp,xgboost</td>\n",
       "      <td>0.080695</td>\n",
       "      <td>0.960053</td>\n",
       "      <td>0.376436</td>\n",
       "      <td>0.910260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>bert,lgbm,mlp,xgboost</td>\n",
       "      <td>0.073597</td>\n",
       "      <td>0.961810</td>\n",
       "      <td>0.379008</td>\n",
       "      <td>0.917727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>bert,mlp,rf</td>\n",
       "      <td>0.022105</td>\n",
       "      <td>0.989267</td>\n",
       "      <td>0.385667</td>\n",
       "      <td>0.845967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>bert,lgbm,mlp,rf</td>\n",
       "      <td>0.022087</td>\n",
       "      <td>0.989258</td>\n",
       "      <td>0.386913</td>\n",
       "      <td>0.845587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>bert,mlp,xgboost</td>\n",
       "      <td>0.074098</td>\n",
       "      <td>0.961255</td>\n",
       "      <td>0.392708</td>\n",
       "      <td>0.915825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>siam_lstm,lgbm,mlp,rf</td>\n",
       "      <td>0.020762</td>\n",
       "      <td>0.990105</td>\n",
       "      <td>0.398626</td>\n",
       "      <td>0.844115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>bert,mlp,rf,xgboost</td>\n",
       "      <td>0.019688</td>\n",
       "      <td>0.990340</td>\n",
       "      <td>0.418745</td>\n",
       "      <td>0.864788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>siam_lstm,lgbm,mlp,rf,xgboost</td>\n",
       "      <td>0.019302</td>\n",
       "      <td>0.990695</td>\n",
       "      <td>0.423117</td>\n",
       "      <td>0.858037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>bert,lgbm,mlp,rf,xgboost</td>\n",
       "      <td>0.019551</td>\n",
       "      <td>0.990357</td>\n",
       "      <td>0.425052</td>\n",
       "      <td>0.863459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>lgbm,mlp,rf</td>\n",
       "      <td>0.025918</td>\n",
       "      <td>0.987067</td>\n",
       "      <td>0.431145</td>\n",
       "      <td>0.833229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>lgbm,mlp,rf,xgboost</td>\n",
       "      <td>0.023345</td>\n",
       "      <td>0.988495</td>\n",
       "      <td>0.468120</td>\n",
       "      <td>0.847799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>rf,xgboost</td>\n",
       "      <td>0.126048</td>\n",
       "      <td>0.934638</td>\n",
       "      <td>0.474674</td>\n",
       "      <td>0.770072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>siam_lstm,mlp,xgboost</td>\n",
       "      <td>0.086443</td>\n",
       "      <td>0.957358</td>\n",
       "      <td>0.526537</td>\n",
       "      <td>0.865190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>siam_lstm,mlp,rf</td>\n",
       "      <td>0.024397</td>\n",
       "      <td>0.988652</td>\n",
       "      <td>0.537223</td>\n",
       "      <td>0.798389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>siam_lstm,mlp,rf,xgboost</td>\n",
       "      <td>0.022293</td>\n",
       "      <td>0.989307</td>\n",
       "      <td>0.557352</td>\n",
       "      <td>0.814222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>mlp,rf</td>\n",
       "      <td>0.037342</td>\n",
       "      <td>0.981237</td>\n",
       "      <td>0.710639</td>\n",
       "      <td>0.746463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>mlp,rf,xgboost</td>\n",
       "      <td>0.031954</td>\n",
       "      <td>0.983736</td>\n",
       "      <td>0.738627</td>\n",
       "      <td>0.771033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>mlp,xgboost</td>\n",
       "      <td>0.155317</td>\n",
       "      <td>0.919136</td>\n",
       "      <td>0.828669</td>\n",
       "      <td>0.784664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Stacked Model  Train Log Loss  Train Precision  \\\n",
       "0                         bert,lgbm        0.125318         0.935907   \n",
       "1                     bert,lgbm,mlp        0.125301         0.935968   \n",
       "2                          bert,mlp        0.133120         0.930157   \n",
       "3                    siam_lstm,bert        0.089873         0.952187   \n",
       "4               siam_lstm,bert,lgbm        0.089812         0.952045   \n",
       "5                          lgbm,mlp        0.131216         0.936443   \n",
       "6           siam_lstm,bert,lgbm,mlp        0.087988         0.954055   \n",
       "7                siam_lstm,bert,mlp        0.088223         0.953479   \n",
       "8            siam_lstm,bert,xgboost        0.081837         0.955377   \n",
       "9                siam_lstm,lgbm,mlp        0.094769         0.953671   \n",
       "10                   siam_lstm,lgbm        0.102490         0.948372   \n",
       "11                     bert,xgboost        0.099880         0.946260   \n",
       "12      siam_lstm,bert,lgbm,xgboost        0.078409         0.957051   \n",
       "13                     lgbm,xgboost        0.120456         0.937038   \n",
       "14                bert,lgbm,xgboost        0.099365         0.946301   \n",
       "15           siam_lstm,lgbm,xgboost        0.098293         0.949370   \n",
       "16                siam_lstm,bert,rf        0.067014         0.964584   \n",
       "17        siam_lstm,bert,rf,xgboost        0.065609         0.965710   \n",
       "18                          bert,rf        0.075601         0.959547   \n",
       "19                  bert,rf,xgboost        0.074951         0.960062   \n",
       "20           siam_lstm,bert,lgbm,rf        0.058722         0.969229   \n",
       "21   siam_lstm,bert,lgbm,rf,xgboost        0.058354         0.969742   \n",
       "22                siam_lstm,lgbm,rf        0.083681         0.956582   \n",
       "23                     bert,lgbm,rf        0.070369         0.963516   \n",
       "24             bert,lgbm,rf,xgboost        0.070287         0.963610   \n",
       "25                          lgbm,rf        0.098333         0.948006   \n",
       "26        siam_lstm,lgbm,rf,xgboost        0.081482         0.958344   \n",
       "27                  lgbm,rf,xgboost        0.096917         0.949378   \n",
       "28                    siam_lstm,mlp        0.146865         0.927355   \n",
       "29                siam_lstm,xgboost        0.121196         0.937322   \n",
       "30       siam_lstm,bert,mlp,xgboost        0.056291         0.970722   \n",
       "31  siam_lstm,bert,lgbm,mlp,xgboost        0.056095         0.970632   \n",
       "32       siam_lstm,lgbm,mlp,xgboost        0.062963         0.968668   \n",
       "33            siam_lstm,bert,mlp,rf        0.017788         0.991536   \n",
       "34                     siam_lstm,rf        0.092429         0.952690   \n",
       "35       siam_lstm,bert,lgbm,mlp,rf        0.017344         0.991769   \n",
       "36             siam_lstm,rf,xgboost        0.090736         0.954233   \n",
       "37    siam_lstm,bert,mlp,rf,xgboost        0.016500         0.991910   \n",
       "38                 lgbm,mlp,xgboost        0.080695         0.960053   \n",
       "39            bert,lgbm,mlp,xgboost        0.073597         0.961810   \n",
       "40                      bert,mlp,rf        0.022105         0.989267   \n",
       "41                 bert,lgbm,mlp,rf        0.022087         0.989258   \n",
       "42                 bert,mlp,xgboost        0.074098         0.961255   \n",
       "43            siam_lstm,lgbm,mlp,rf        0.020762         0.990105   \n",
       "44              bert,mlp,rf,xgboost        0.019688         0.990340   \n",
       "45    siam_lstm,lgbm,mlp,rf,xgboost        0.019302         0.990695   \n",
       "46         bert,lgbm,mlp,rf,xgboost        0.019551         0.990357   \n",
       "47                      lgbm,mlp,rf        0.025918         0.987067   \n",
       "48              lgbm,mlp,rf,xgboost        0.023345         0.988495   \n",
       "49                       rf,xgboost        0.126048         0.934638   \n",
       "50            siam_lstm,mlp,xgboost        0.086443         0.957358   \n",
       "51                 siam_lstm,mlp,rf        0.024397         0.988652   \n",
       "52         siam_lstm,mlp,rf,xgboost        0.022293         0.989307   \n",
       "53                           mlp,rf        0.037342         0.981237   \n",
       "54                   mlp,rf,xgboost        0.031954         0.983736   \n",
       "55                      mlp,xgboost        0.155317         0.919136   \n",
       "\n",
       "    Test Log Loss  Test Precision  \n",
       "0        0.133022        0.930700  \n",
       "1        0.133088        0.930564  \n",
       "2        0.135653        0.927189  \n",
       "3        0.143896        0.928853  \n",
       "4        0.144577        0.928110  \n",
       "5        0.144879        0.927349  \n",
       "6        0.145437        0.928533  \n",
       "7        0.146651        0.927930  \n",
       "8        0.157603        0.935726  \n",
       "9        0.159301        0.922521  \n",
       "10       0.164741        0.916412  \n",
       "11       0.172028        0.934690  \n",
       "12       0.173858        0.934537  \n",
       "13       0.180276        0.921289  \n",
       "14       0.180886        0.934214  \n",
       "15       0.182577        0.917744  \n",
       "16       0.202771        0.905172  \n",
       "17       0.211773        0.892403  \n",
       "18       0.219718        0.894265  \n",
       "19       0.225169        0.886706  \n",
       "20       0.232920        0.893046  \n",
       "21       0.235411        0.887339  \n",
       "22       0.250067        0.876565  \n",
       "23       0.250165        0.880541  \n",
       "24       0.250928        0.877893  \n",
       "25       0.257836        0.872240  \n",
       "26       0.261450        0.864046  \n",
       "27       0.265358        0.862398  \n",
       "28       0.293724        0.849593  \n",
       "29       0.299555        0.873506  \n",
       "30       0.317810        0.929912  \n",
       "31       0.324254        0.929823  \n",
       "32       0.326607        0.918919  \n",
       "33       0.346760        0.862448  \n",
       "34       0.349740        0.829694  \n",
       "35       0.351390        0.860478  \n",
       "36       0.364152        0.816474  \n",
       "37       0.369391        0.878779  \n",
       "38       0.376436        0.910260  \n",
       "39       0.379008        0.917727  \n",
       "40       0.385667        0.845967  \n",
       "41       0.386913        0.845587  \n",
       "42       0.392708        0.915825  \n",
       "43       0.398626        0.844115  \n",
       "44       0.418745        0.864788  \n",
       "45       0.423117        0.858037  \n",
       "46       0.425052        0.863459  \n",
       "47       0.431145        0.833229  \n",
       "48       0.468120        0.847799  \n",
       "49       0.474674        0.770072  \n",
       "50       0.526537        0.865190  \n",
       "51       0.537223        0.798389  \n",
       "52       0.557352        0.814222  \n",
       "53       0.710639        0.746463  \n",
       "54       0.738627        0.771033  \n",
       "55       0.828669        0.784664  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result_df = pd.Dataframe(columns = ['Input', 'Train Log Loss', 'Trai Precision', 'Test Log Loss', 'Test Precision'])\n",
    "result_list = []\n",
    "log_clf = LogisticRegression()\n",
    "for i in input_list:\n",
    "    subset = i.split(\",\")\n",
    "    train_subset = X_train[subset]\n",
    "    test_subset = X_test[subset]\n",
    "    log_clf.fit(train_subset,y_train)\n",
    "    preds_train = log_clf.predict(train_subset)\n",
    "    preds_prob_train = log_clf.predict_proba(train_subset)\n",
    "    preds_test = log_clf.predict(test_subset)\n",
    "    preds_prob_test = log_clf.predict_proba(test_subset)\n",
    "    l = [i, log_loss(y_train, preds_prob_train),precision_score(y_train, preds_train), \n",
    "                       log_loss(y_test, preds_prob_test), precision_score(y_test, preds_test)]\n",
    "    result_list.append(l)\n",
    "\n",
    "result_df = pd.DataFrame(result_list)\n",
    "result_df.columns = ['Stacked Model','Train Log Loss', 'Train Precision', 'Test Log Loss', 'Test Precision']\n",
    "result_df = result_df.sort_values(by =['Test Log Loss'], ascending = True)\n",
    "result_df.reset_index(drop = True, inplace = True)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c8a7a",
   "metadata": {},
   "source": [
    "# 3. Best Subset Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54e6183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_subset_feature = result_df['Stacked Model'].values[0].split(\",\")\n",
    "X_train = X_train[best_subset_feature]\n",
    "X_test = X_test[best_subset_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d4f318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf.fit(X_train, y_train)\n",
    "preds_train = log_clf.predict(X_train)\n",
    "preds_prob_train = log_clf.predict_proba(X_train)\n",
    "preds_test = log_clf.predict(X_test)\n",
    "preds_prob_test = log_clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6203eaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train log loss is: 0.12531842132839877\n",
      "The train precision is: 0.9359066709721222\n",
      "The test log loss is: 0.13302161760440695\n",
      "The test precision is: 0.9307002505604642\n"
     ]
    }
   ],
   "source": [
    "print(\"The train log loss is:\", log_loss(y_train, preds_prob_train))\n",
    "print(\"The train precision is:\", precision_score(y_train, preds_train))\n",
    "print(\"The test log loss is:\", log_loss(y_test, preds_prob_test))\n",
    "print(\"The test precision is:\", precision_score(y_test, preds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84de69d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
